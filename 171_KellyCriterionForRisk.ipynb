{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe5d91da",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#000;\"><img src=\"pqn.png\"></img></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77599526",
   "metadata": {},
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef682ac",
   "metadata": {},
   "source": [
    "We use NumPy for math, pandas for working with tables, matplotlib for charting, yfinance for getting financial prices, and SciPy for optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9db97a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7d0ab4",
   "metadata": {},
   "source": [
    "This code fixes the seed so that we get the same results each time we run the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae023b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847ff77",
   "metadata": {},
   "source": [
    "Here, we define how many stocks we want, how much data to look at, and which companies we’re studying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02672fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_assets = 5\n",
    "n_obs = 750\n",
    "symbols = [\"AAPL\", \"MSFT\", \"GOOGL\", \"META\", \"NVDA\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e355ae",
   "metadata": {},
   "source": [
    "We use yfinance to pull closing stock prices, adjust for any splits/dividends, and calculate daily log returns for our assets over the last three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74865ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = yf.download(\n",
    "    symbols, period=\"3y\", interval=\"1d\", progress=False, auto_adjust=True\n",
    ")[\"Close\"].dropna()\n",
    "returns = np.log(data).diff().dropna().iloc[-n_obs:]\n",
    "mean_return_annual = returns.mean() * 252\n",
    "cov_annual = returns.cov() * 252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3251a5",
   "metadata": {},
   "source": [
    "In this first section, we're building the groundwork for our analysis. We set up our scientific tools, lock in reproducibility, select our stocks, and pull historical price data from Yahoo Finance. Then, we convert those prices to returns, and prepare averages and risk levels we’ll need to assess different portfolios. These calculations ensure all later steps use realistic and consistent data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a5097",
   "metadata": {},
   "source": [
    "## Simulate synthetic return scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a07ae78",
   "metadata": {},
   "source": [
    "Here, we set up numbers to use in case real data is missing, then calculate average returns, risk (volatility), and relationships between each stock. We use these to randomize new returns similar to what we might see in real life. This gives us clean data for building our portfolios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5446aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "drift = 0.10\n",
    "mu = (\n",
    "    mean_return_annual.values\n",
    "    if not np.any(np.isnan(mean_return_annual))\n",
    "    else np.full(n_assets, drift)\n",
    ")\n",
    "sigma = (\n",
    "    np.sqrt(np.diag(cov_annual))\n",
    "    if not np.any(np.isnan(cov_annual))\n",
    "    else np.full(n_assets, 0.20)\n",
    ")\n",
    "corr_matrix = (\n",
    "    cov_annual.corr().values if not np.any(np.isnan(cov_annual)) else np.eye(n_assets)\n",
    ")\n",
    "cov = np.outer(sigma, sigma) * corr_matrix\n",
    "synthetic_returns = np.random.multivariate_normal(mu / 252, cov / 252, size=n_obs)\n",
    "synthetic_returns = pd.DataFrame(synthetic_returns, columns=symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab2fb7",
   "metadata": {},
   "source": [
    "This block builds a simulated world where every stock's return matches the math from our real-world estimates. If there's missing info, we fall back on rough assumptions so everything stays robust. We end up with a fresh batch of data that mimics real returns, set up in a table just like what you'd get from actual market prices. This synthetic data helps us control for outliers and makes the modeling straightforward and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500f2555",
   "metadata": {},
   "source": [
    "Here, we get rid of any days that look too different from the rest, cleaning the data so a handful of wild numbers don’t throw off our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb69fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_thresh = 4\n",
    "mean_return = synthetic_returns.mean()\n",
    "std_return = synthetic_returns.std()\n",
    "non_outlier_idx = (\n",
    "    (synthetic_returns - mean_return).abs() <= outlier_thresh * std_return\n",
    ").all(axis=1)\n",
    "filtered_returns = synthetic_returns[non_outlier_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebf5d84",
   "metadata": {},
   "source": [
    "We filter out any unusually large or small simulated returns that fall too far from the average pattern. This helps keep our later results reliable and not influenced by extreme, rare events. After this cleaning, only the most representative return scenarios remain, making our future calculations much more dependable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd7759",
   "metadata": {},
   "source": [
    "From this cleaner data, we recalculate the realistic, annualized average returns and risk for each stock so we can use them in portfolio construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36060df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_mu = filtered_returns.mean().values * 252\n",
    "sim_cov = filtered_returns.cov().values * 252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c5e5c",
   "metadata": {},
   "source": [
    "Now, with extreme outliers removed, we calculate new estimates for what the stocks might earn and how their risks interact. We annualize the results so everything is on a common scale for use in later portfolio math. This ensures that our inputs better reflect what an investor might actually experience, without distortion from wild, one-off days."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c37585",
   "metadata": {},
   "source": [
    "## Build and evaluate portfolios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385aca8",
   "metadata": {},
   "source": [
    "We randomly mix the five stocks into 400 different portfolios, then calculate expected return and risk for each one based on our cleaned estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73c3f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 400\n",
    "rand_weights = np.random.dirichlet(np.ones(n_assets), size=n_samples)\n",
    "rand_port_rets = rand_weights @ sim_mu\n",
    "rand_port_vols = np.sqrt(np.einsum(\"ij,jk,ik->i\", rand_weights, sim_cov, rand_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f659248",
   "metadata": {},
   "source": [
    "By creating many random mixes, we can see what’s possible if you just pick different asset weights at random. We calculate how much you could expect to gain and how much volatility you’d take on for each approach. This gives us a full map of possible outcomes across the landscape of diversified portfolios, all based on consistent risk and return numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c73b619",
   "metadata": {},
   "source": [
    "We prepare the return and risk numbers we just estimated so they can be used for designing portfolios in the next block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b32b358",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_mu = sim_mu\n",
    "mat_cov = sim_cov"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca02883c",
   "metadata": {},
   "source": [
    "This step just saves our newly cleaned and annualized averages and risk calculations, which we’ll use as official inputs. This keeps everything tidy and avoids confusion as we try new methods to optimize these portfolios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fd8c50",
   "metadata": {},
   "source": [
    "Here, we plot out the full range of optimal risk-return combinations (“efficient frontier”) by asking: for each risk level, what’s the best portfolio we can construct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251b551c",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_aversion_grid = np.linspace(0.0001, 10, 40)\n",
    "ef_returns = []\n",
    "ef_vols = []\n",
    "ef_weights = []\n",
    "for gamma in risk_aversion_grid:\n",
    "\n",
    "    def obj(w):\n",
    "        return -w @ mat_mu + gamma * 0.5 * w @ mat_cov @ w\n",
    "\n",
    "    cons = [\n",
    "        {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1},\n",
    "        {\"type\": \"ineq\", \"fun\": lambda w: w},\n",
    "    ]\n",
    "    bounds = [(0, 1) for _ in range(n_assets)]\n",
    "    res = minimize(\n",
    "        obj,\n",
    "        np.ones(n_assets) / n_assets,\n",
    "        method=\"SLSQP\",\n",
    "        constraints=cons,\n",
    "        bounds=bounds,\n",
    "        options={\"disp\": False},\n",
    "    )\n",
    "    x = res.x\n",
    "    ef_weights.append(x)\n",
    "    ef_returns.append(x @ mat_mu)\n",
    "    ef_vols.append(np.sqrt(x @ mat_cov @ x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97950886",
   "metadata": {},
   "source": [
    "We methodically adjust the balance between wanting more return and wanting less risk. For each point along that trade-off, we use optimization from SciPy to find the best way to split our money across the stocks. Our process always requires we invest every dollar and don’t short sell. This creates a smooth curve of optimal portfolios, showing exactly what’s possible from best to most conservative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414d6d4",
   "metadata": {},
   "source": [
    "Now we create the “Kelly curve,” a special set of portfolios that aim to maximize how quickly our wealth could grow if returns stay consistent. This method doesn’t limit portfolio weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459685d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_cov = np.linalg.pinv(mat_cov)\n",
    "kelly_w = inv_cov @ mat_mu\n",
    "kelly_scale = np.linspace(0, 2, 80)\n",
    "kelly_port_returns = [scale * kelly_w @ mat_mu for scale in kelly_scale]\n",
    "kelly_port_vols = [\n",
    "    np.sqrt((scale * kelly_w) @ mat_cov @ (scale * kelly_w)) for scale in kelly_scale\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ac70bd",
   "metadata": {},
   "source": [
    "We use a mathematical shortcut to find one particular way to invest that should maximize growth over the long run. By scaling that approach up and down, we see what happens when you take on more or less risk. Unlike our earlier optimized portfolios, these portfolios ignore any limits on where your money can go, giving us another view on the risk-return space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf3312b",
   "metadata": {},
   "source": [
    "## Visualize the portfolio risk and return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ef403",
   "metadata": {},
   "source": [
    "We make a chart showing every random portfolio, the optimal line of portfolios, and the Kelly curve. This lets us compare approaches and see how different strategies relate in terms of risk and expected return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd2581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "plt.scatter(\n",
    "    rand_port_vols,\n",
    "    rand_port_rets,\n",
    "    c=\"lightgray\",\n",
    "    label=\"Random Portfolios\",\n",
    "    s=20,\n",
    "    alpha=0.7,\n",
    ")\n",
    "plt.plot(ef_vols, ef_returns, color=\"navy\", lw=2, label=\"Efficient Frontier\")\n",
    "plt.plot(kelly_port_vols, kelly_port_returns, color=\"green\", lw=2, label=\"Kelly Curve\")\n",
    "plt.xlabel(\"Annualized Volatility\")\n",
    "plt.ylabel(\"Annualized Return\")\n",
    "plt.title(\"Random Portfolios, Efficient Frontier, and Kelly Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.2)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97780cb",
   "metadata": {},
   "source": [
    "Our final step visualizes everything we’ve built. Each gray dot happens when we throw stock picks together at random, while the navy line shows the best you can do for any risk level. The green curve marks the theoretically highest-growth approaches. By comparing the dots, the line, and the curve, we see the range of choices in managing risk versus reward. The plot gives us a clear, direct look at how our portfolio-building strategy shapes real investment possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e9ef53",
   "metadata": {},
   "source": [
    "<a href=\"https://pyquantnews.com/\">PyQuant News</a> is where finance practitioners level up with Python for quant finance, algorithmic trading, and market data analysis. Looking to get started? Check out the fastest growing, top-selling course to <a href=\"https://gettingstartedwithpythonforquantfinance.com/\">get started with Python for quant finance</a>. For educational purposes. Not investment advice. Use at your own risk."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
